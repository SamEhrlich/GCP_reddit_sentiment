{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses GCP's VM instance to scrape reddit's RSS feed into GCP's NLP. Then the data that is scraped along with the processed text is transferred into a PostgreSQL database held on GCP's platform. Then Python is used to connect to the database, query the database and visualize the data. \n",
    "\n",
    "Step 1 is to create a VM instance through the compute engine tab on the GCP site\n",
    "Step 2 is to allow the VM access to the NLP and begin creating a script within the VM using vim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--code to setup VM with python, pip and packages for scraping (This assumes a VM and a bucket is created under the same project)\n",
    "\n",
    "sudo apt update\n",
    "sudo apt install python3-venv python3-pip\n",
    "\n",
    "pip3 install feedparser\n",
    "pip3 install google-cloud\n",
    "pip3 install google-cloud-vision\n",
    "pip3 install --upgrade google-cloud-storage\n",
    "pip3 install google-cloud-language\n",
    "\n",
    "--allow google API to access VM\n",
    "\n",
    "gcloud auth application-default login\n",
    "\n",
    "--create script to scrape \n",
    "\n",
    "vim name_of_script.py\n",
    "\n",
    "--functions inside vim\n",
    "\n",
    "press \"i\" to insert code into vim and \"esc\" to exit insert mode\n",
    ":w --writes \n",
    ":q --quits out of script and back to VM\n",
    ":wq --writes and then quits\n",
    ":!python3 name_of_script.py --runs script within vim OR outside of vim just type 'python3 name_of_script.py'\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " --script used inside of VM to scrape reddit RSS feed into NLP\n",
    "\n",
    "import feedparser #parses rss feeds including twitter/fb/reddit\n",
    "from bs4 import BeautifulSoup #scrape data\n",
    "from bs4.element import Comment \n",
    "from google.protobuf.json_format import MessageToDict #change json format from NLP into dictionary format\n",
    "from google.cloud import language #NLP\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "--get only the visible text elements from the feed\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "--get the body from the html\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "--url of reddit (get 100 most recent posts)\n",
    "a_reddit_rss_url = 'https://www.reddit.com/r/leagueoflegends/.rss?limit=100&after=t3_zms68r'\n",
    "feed = feedparser.parse(a_reddit_rss_url)\n",
    "\n",
    "--initialize an empty list to store data coming in\n",
    "posts = []\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")\n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        date = item[ \"date\" ][0:10]\n",
    "        title = item[ \"title\" ]\n",
    "        summary_text = text_from_html(item[ \"summary\" ])\n",
    "        link = item[ \"link\" ]\n",
    "        lang = language.LanguageServiceClient()\n",
    "        document = language.Document(content = summary_text, type_ = language.Document.Type.PLAIN_TEXT)\n",
    "        response = lang.analyze_sentiment(document = document)\n",
    "        result_dict = MessageToDict(response._pb)\n",
    "        result_dict['documentSentiment'].setdefault('score',0) #if no score is listed set the default to 0\n",
    "        result_dict['documentSentiment'].setdefault('magnitude',0) #if no magnitude is listed set the default to 0\n",
    "        d = {'date' : date, 'title' : title, 'text' : summary_text, 'link' : link,\n",
    "            'sent_score' : result_dict['documentSentiment']['score'],\n",
    "            'magnitude' : result_dict['documentSentiment']['magnitude']}\n",
    "        posts.append(d)\n",
    "        \n",
    "--initialize variables for connecting and loading data into database\n",
    "project_id = 'project_id'\n",
    "region = 'us_central1'\n",
    "instance_name = 'instance_name'\n",
    "\n",
    "INSTANCE_CONNECTION_NAME = f\"{project_id}:{region}:{instance_name}\"\n",
    "\n",
    "DB_USER = 'sam_ehrlich'\n",
    "DB_PASS = 'postgres'\n",
    "DB_NAME = 'name_of_db'\n",
    "\n",
    "--connect and load data into database using sqlalchemy\n",
    "\n",
    "from google.cloud.sql.connector import Connector\n",
    "import sqlalchemy\n",
    "\n",
    "--initialize connector\n",
    "connector = Connector()\n",
    "\n",
    "--function to return database connector object\n",
    "def getconn():\n",
    "    conn =  connector.connect(\n",
    "        INSTANCE_CONNECTION_NAME,\n",
    "        \"pg8000\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        db=DB_NAME,\n",
    "        )\n",
    "    return conn\n",
    "\n",
    "--create connection pool with 'creator' argument to our connection object function\n",
    "pool = sqlalchemy.create_engine(\n",
    "    \"postgresql+pg8000://\",\n",
    "    creator=getconn,\n",
    ")\n",
    "\n",
    "--connect and create table \n",
    "with pool.connect() as db_conn:\n",
    "    db.conn.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS ratings \"\n",
    "    \"(id SERIAL NOT NULL, date VARCHAR(12), title VARCHAR(256) NOT NULL, \"\n",
    "    \"text TEXT, link VARCHAR(255), sent_score FLOAT, magnitude FLOAT \"\n",
    "    \"PRIMARY KEY (id));\"\n",
    "    )\n",
    "    \n",
    "--insert data into our new table\n",
    "    insert_statement = sqlalchemy.text(\n",
    "    \"INSERT INTO ratings (date, title, text, link, sent_score, magnitude) VALUES \\\n",
    "    (:date, :title, :text, :link, :sent_score, :magnitude)\",\n",
    "    )\n",
    "    \n",
    "    for each in posts:\n",
    "        db.conn.execute(insert_statement, date = each['date'], title = each['title'], \\\n",
    "        text = each['text'], link = each['link'], sent_score = each['sent_score'], \\\n",
    "        magnitude = each['magnitude'])\n",
    "\n",
    "--query database\n",
    "    results = db.conn.execute(\"SELECT * FROM ratings LIMIT 5;\").fetchall()\n",
    "\n",
    "--print results of query\n",
    "    for row in results:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been scraped and loaded from within the VM, anyone with a connection to the database can query the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to db through jupyter notebook\n",
    "import getpass\n",
    "mypasswd = getpass.getpass()\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "connection = psycopg2.connect(database = 'db_name', \n",
    "                              user = 'user', \n",
    "                              host = '99.999.999.99', # Replace with SQL IP\n",
    "                              password = 'password')\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT * FROM ratings\")\n",
    "    results = cursor.fetchall()\n",
    "    for row in results:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query db using pandas\n",
    "import pandas as pd\n",
    "\n",
    "SQL = \"SELECT * \"\n",
    "SQL += \" FROM reddit_table \"\n",
    "SQL += \" \"\n",
    "\n",
    "print(SQL)\n",
    "reddit_df = pd.read_sql(SQL,connection)\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date to datetime object\n",
    "import datetime as dt\n",
    "\n",
    "reddit_df['date'] = pd.to_datetime(reddit_df['date'], format ='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign a net positive/negative/neutral tag to each post\n",
    "reddit_df['sentiment'] = 'NEU'\n",
    "reddit_df.loc[reddit_df['sent_score'] >= 0.25, 'sentiment'] = 'POS'\n",
    "reddit_df.loc[reddit_df['sent_score'] <= -0.25, 'sentiment'] = 'NEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize distribution of post's sentiment\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x=reddit_df[\"sentiment\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
